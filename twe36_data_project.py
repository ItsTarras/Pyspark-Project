# -*- coding: utf-8 -*-
"""twe36 Data Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aTAzLSQSDwj6b9yTnYNDBa3jJXufZCKc

#Outfit analysis project
For this study, I will analyse clothing ratings, and the most important keywords associated for each type of outfit.

Method: The algorithms used in this will consist of the following:
Map-Reduce: for counting the number of words in each review type.
IDF and TDIF: For counting the importance of words and their confidences.

##Download data
"""

import urllib.request
filename = 'renttherunway_final_data.json'
urllib.request.urlretrieve('https://drive.google.com/uc?export=download&confirm=t&id=1zDYQ1DG5K4VZ46MpJMnFykc7dQRJnwDp', filename)

"""##Install ijson and pyspark"""

#Download Stopwords importation



!pip install nltk

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

STOP_WORDS = set(stopwords.words('english'))
print(STOP_WORDS)

!pip install pyspark
print("Successfully installed pyspark.")

"""##Import the modules, and establish a Spark session"""

#Imports the json files and decompresses them. This block also installs most
# of the required modules and libraries.

#Import modules
import json, gzip

#Import pyspark and the configuration module (SparkConf) for memory management.
from pyspark import SparkContext, SparkConf

conf = SparkConf().setAppName("my_app").set("spark.executor.memory", "4g")
sc = SparkContext(conf=conf)

"""#Read JSON file & RDD Establishment
This code cell reads the JSON file, decodes it, and turns it into a parallelized RDD.
"""

#Uses gzip to take the file and reads it as a binary file.
#From there, the file is decoded, parallelized and split by lines.

the_file = gzip.GzipFile(filename, 'rb')
data = the_file.read().decode('utf-8')
full_file = data.splitlines()
rdd = sc.parallelize(full_file)



#Debugging line:
text_file = sc.textFile(filename)
# force spark to load the file
print(f"{filename} loaded with {text_file.count()} lines")

#Debugging line
print(rdd.take(1))

"""##Transform the RDD and create Maps"""

#Create a map to use, based on all unique "rented for" tags.

#Creates a helper function to return a string of positive, negative or mixed
def rate(number):
  """Returns a rating of positive, negative or mixed, based on a rating value"""
  if number in ["0", "1", "2", "3"] or number in [0, 1, 2, 3]:
    return "Negative"
  elif number in ["4", "5", "6"] or number in [4, 5, 6]:
    return "Mixed"
  else:
    #Number is in ["7", "8", "9", "10"]
    return "Positive"


def rate_occasion(line):
  """Returns a rating of positive, negative or mixed, based on a rating value"""
  number = line[1]
  if number in ["0", "1", "2", "3"] or number in [0, 1, 2, 3]:
    return (line[0], "Negative")
  elif number in ["4", "5", "6"] or number in [4, 5, 6]:
    return (line[0], "Mixed")
  else:
    #Number is in ["7", "8", "9", "10"]
    return (line[0], "Positive")

def count_tuple(row_then_words):
    """Returns a word as a tuple with a count when it is assembled."""
    word_counts = []
    word = ""
    for item in row_then_words[1]:
        if item.isalpha() or item == "'":
            word += item.lower()
        elif word:
            if word in STOP_WORDS:
              word = ""
            else:
              word_counts.append(((row_then_words[0], word), 1))
              word = ""
    return word_counts

def count_tuple_and_rating(row_then_words):
    """Returns a word as a tuple with a count when it is assembled."""
    word_counts = []
    word = ""
    for item in row_then_words[1]:
        if item.isalpha() or item == "'":
            word += item.lower()
        elif word:
          if word in STOP_WORDS:
            word = ""
          else:
            #Appends a tuple in the format (Reason, word, rating).
            word_counts.append(((row_then_words[0], word, rate(row_then_words[2])), 1))
            word = ""
    return word_counts


##--------------------------------------------------------------------------------#

# Start by creating an RDD for each review text and the reason it was rented for.


#Creates an rdd with a tuple consisting of the following json lines:
#Json.loads grabs the associated key values out, and appends them to a tuple.
#If `rented for` is not found, it will not use the line
rented_for_counts = rdd.map(lambda x: (json.loads(x).get('rented for', None), json.loads(x).get('review_text', None)))
#print(rented_for_counts.take(10))



#We then need to count the number of words in each rented for key group.
final_rdd = rented_for_counts.flatMap(lambda line: count_tuple(line))
#print(final_rdd.take(6))



#This line takes the longest, so separate other transformations to below:
combined_rdd = final_rdd.reduceByKey(lambda x, y: x + y)
print(combined_rdd.take(10))

#Turn it into a total word count map we can reference later.
total_word_map = sc.broadcast(combined_rdd.collectAsMap())



#Create a map for each occasion and the number of total ratings it has:

#First, give how many positive, negatives, and neutrals there are total., based on
#occasions.

#Gives the total number of (occasion, rating) tuples
rented_plus_rating = rdd.map(lambda x: (json.loads(x).get('rented for', None), json.loads(x)["rating"]))
total_ratings = rented_plus_rating.map(lambda x: (rate_occasion(x), 1 ))

#Gives the total number of occasions, and the number of them equals the
#number of total ratings within the occasion. This is our J
just_occasion = rdd.map(lambda x: (json.loads(x).get('rented for', None)))
just_occasion = just_occasion.map(lambda x: (x, 1))


#Reduces both down into readable formats
final_num_rating = total_ratings.reduceByKey(lambda x, y: x + y)
total_occasion_count = just_occasion.reduceByKey(lambda x, y: x + y)

#Prints a result
#print(final_num_rating.take(10))
#print(total_occasion_count.take(10))


#Broadcast the results as a map
total_rating_map = sc.broadcast(final_num_rating.collectAsMap())
total_occasion_count_map = sc.broadcast(total_occasion_count.collectAsMap())

"""#Grouping word counts by their rented reason

"""

#Group the tuple pair counts by their rented_for reason.
grouped_rdd = combined_rdd.groupBy(lambda pair_count_tuple: pair_count_tuple[0][0])
grouped_list = (grouped_rdd.take(20))
print(grouped_list)

"""The above line is mainly for performance testing

##Continue reducing, and instantiate a new helper function that allows us to gather confidences.
"""

#Helper function to tuple map the confidences of a particular word.
def process(row):
    """Takes a row, and processes it from a map, returning confidences."""
    lists = []

    #We will get an input in the format (('occasion', 'word', 'rating',), count)
    occasion = row[0][0]
    word = row[0][1]
    rating = row[0][2]

    #Count is the number of words and "rating" in each dataset
    count = row[1]

    #Final count calculates this, devided by the total number of the word occurrence.
    final_count = count / total_word_map.value[(occasion, word)]

    #Append the result to a list for flatMapping:
    lists.append(((occasion, word, rating), final_count))

    return lists


#Create a new RDD.
#Using the word, check the number of words that are split by within each reason, "positive", "negative", and "neutral".
#From there, compare that number, with the total number.

rented_and_rating = rdd.map(lambda x: (json.loads(x).get('rented for', None), json.loads(x)['review_text'], json.loads(x)["rating"]))

#We then need to count the number of words in each rented for key group.
rating_RDD = rented_and_rating.flatMap(lambda line: count_tuple_and_rating(line))

# Next, reduce them.
reduced_rdd = rating_RDD.reduceByKey(lambda x, y: x + y)

# Rearrange the tuples to the desired format

#Create an RDD with all 3 variables, and the count of the word in question.
result_rdd = reduced_rdd.map(lambda x: ((x[0][0], x[0][1], x[0][2]), x[1]))

#Helper function to calculate Interests
def calculate_interest(row):
  lists = []
  row = row[0]
  key = (row[0][0], row[0][2])
  occasion = row[0][0]
  confidence = row[1]
  word = row[0][1]
  rating = row[0][2]

  #Get the total number
  total_amount = total_rating_map.value[key]

  #this is given by the confidence minus the total number of J (ratings) from
  #our total set of occasions. So if occasion 'party' has 600/1000 of its ratings
  #as positive, we get a value of 0.6. For this, we will need a map.
  interest = confidence - (total_rating_map.value[key] / total_occasion_count_map.value[occasion])
  lists.append(((occasion, word, rating), interest))
  return lists

#Now compare them in a new RDD to get the interests of each word within its own subset of ratings.

#Returns only the most frequent words. This will try to avoid single misspelt words
resulting_rdd = result_rdd.filter(lambda x: x[1] > 3)

#Calculates confidences
result_confidence = resulting_rdd.map(lambda row: process(row))
print(result_confidence.take(30))

#I will now need to calculate the interest of each word for their occasiona nd rating.
#This is given by the confidence - num(ratings) in all sets.

result_interest = result_confidence.map(lambda x: calculate_interest(x))
print(result_interest.take(10))

#Sorts by interests
sorted_rdd = result_interest.sortBy(lambda x: -x[0][1])

#Debugging line. Print out an ordered list of most interesting words, but not
#grouped yet.
results_q2a = sorted_rdd.takeOrdered(80, lambda kv: kv[0][0][2])
print(results_q2a)

"""#Group the results by their occasion, and print the resulting valuable keywords."""

# Group the RDD elements by occasion
grouped_rdd = sorted_rdd.groupBy(lambda x: (x[0][0][0], x[0][0][2]))
#print(grouped_rdd.take(10))

# Define the order of ratings
rating_order = ['Positive', 'Mixed', 'Negative']

# Define a helper function to get the index of a rating
def get_rating_index(rating):
    return rating_order.index(rating)

# Sorts groupedRDD by the index (Start with positive, then move down) (Due to spelling
#And the spelling would not sort well).
sorted_elements = sorted(grouped_rdd.collect(), key=lambda x: get_rating_index(x[0][1]))

# Iterate over the sorted elements and print the results
for (occasion, rating), elements in sorted_elements:
    # If our dataset contains less than 10 interesting and unique words, it is not going to be useful to us in the slightest, so only print useful ones.
    if len(elements) > 10:
        print("Occasion:", occasion.capitalize(), "-", rating)
        top_10_elements = sorted(elements, key=lambda x: x[0][1], reverse=True)[:10]

        for element in top_10_elements:
            print("Word:", element[0][0][1])

    print()

"""As we can see from the results above, the information that is siphoned out contains quite a few junk files. This may be due to things such as differing language and adjectives used in leau of actually useful words and nouns.

I also changed the threshold for number of required occurences to be above 2 This resulted in more valuable words being used, because it removes words that may appear once or twice since they are probably misspelt.

We can see that the returned words are pretty on-topic, but not all of the information is completely accurate.
Some keywords are relevant, but others such as "shame", "unflattering" and disappointing are negative and don't provide much information about their issue.

For a much more comprehesive analysis, a larger dataset would be useful, because some occasions have very small subsets, leading to inaccurate or non-useful results.

#Google Cloud establishment
"""

# Commented out IPython magic to ensure Python compatibility.
USERNAME="twe36"
# %env REGION=australia-southeast1
# %env ZONE=australia-southeast1-a
# %env PROJECT=$USERNAME-data301-project
# %env CLUSTER=$USERNAME-data301-project-cluster
# %env BUCKET=$USERNAME-data301-project-bucket

"""#Set up project and storage bucket"""

!python3 -m pip install google-cloud-dataproc[libcst]

"""#Login"""

!gcloud auth login

!gcloud config set project "twe36-data301-project"

!gcloud services enable dataproc.googleapis.com cloudresourcemanager.googleapis.com

!gsutil mb -c regional -l $REGION -p "twe36-data301-project" gs://$BUCKET

!gcloud storage cp ./renttherunway_final_data.json gs://$BUCKET

"""#The whole code snippet that may need to be reduced down."""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile pyspark_data_project.py
# import urllib.request
# 
# filename = 'renttherunway_final_data.json'
# urllib.request.urlretrieve('https://drive.google.com/uc?export=download&confirm=t&id=1zDYQ1DG5K4VZ46MpJMnFykc7dQRJnwDp', filename)
# 
# #Download Stopwords importation
# import subprocess
# # Install nltk package using pip
# subprocess.check_call(['pip', 'install', 'nltk'])
# 
# 
# import nltk
# nltk.download('stopwords')
# 
# from nltk.corpus import stopwords
# 
# STOP_WORDS = set(stopwords.words('english'))
# print(STOP_WORDS)
# 
# print("Successfully installed pyspark.")
# 
# 
# #Imports the json files and decompresses them. This block also installs most
# # of the required modules and libraries.
# 
# #Import modules
# import json, gzip
# 
# #Import pyspark and the configuration module (SparkConf) for memory management.
# from pyspark import SparkContext, SparkConf
# 
# conf = SparkConf().setAppName("my_app").set("spark.executor.memory", "2g")
# sc = SparkContext(conf=conf)
# 
# #Uses gzip to take the file and reads it as a binary file.
# #From there, the file is decoded, parallelized and split by lines.
# 
# the_file = gzip.GzipFile(filename, 'rb')
# data = the_file.read().decode('utf-8')
# full_file = data.splitlines()
# rdd = sc.parallelize(full_file)
# 
# 
# 
# #Debugging line:
# #text_file = sc.textFile(filename)
# # force spark to load the file
# #print(f"{filename} loaded with {text_file.count()} lines")
# 
# 
# 
# 
# 
# #Begin reading the actual code, and test it for timings.
# #Create a map to use, based on all unique "rented for" tags.
# 
# #Creates a helper function to return a string of positive, negative or mixed
# def rate(number):
#   """Returns a rating of positive, negative or mixed, based on a rating value"""
#   if number in ["0", "1", "2", "3"] or number in [0, 1, 2, 3]:
#     return "Negative"
#   elif number in ["4", "5", "6"] or number in [4, 5, 6]:
#     return "Mixed"
#   else:
#     #Number is in ["7", "8", "9", "10"]
#     return "Positive"
# 
# 
# def rate_occasion(line):
#   """Returns a rating of positive, negative or mixed, based on a rating value"""
#   number = line[1]
#   if number in ["0", "1", "2", "3"] or number in [0, 1, 2, 3]:
#     return (line[0], "Negative")
#   elif number in ["4", "5", "6"] or number in [4, 5, 6]:
#     return (line[0], "Mixed")
#   else:
#     #Number is in ["7", "8", "9", "10"]
#     return (line[0], "Positive")
# 
# def count_tuple(row_then_words):
#     """Returns a word as a tuple with a count when it is assembled."""
#     word_counts = []
#     word = ""
#     for item in row_then_words[1]:
#         if item.isalpha() or item == "'":
#             word += item.lower()
#         elif word:
#             if word in STOP_WORDS:
#               word = ""
#             else:
#               word_counts.append(((row_then_words[0], word), 1))
#               word = ""
#     return word_counts
# 
# def count_tuple_and_rating(row_then_words):
#     """Returns a word as a tuple with a count when it is assembled."""
#     word_counts = []
#     word = ""
#     for item in row_then_words[1]:
#         if item.isalpha() or item == "'":
#             word += item.lower()
#         elif word:
#           if word in STOP_WORDS:
#             word = ""
#           else:
#             #Appends a tuple in the format (Reason, word, rating).
#             word_counts.append(((row_then_words[0], word, rate(row_then_words[2])), 1))
#             word = ""
#     return word_counts
# 
# 
# ##--------------------------------------------------------------------------------#
# 
# # Start by creating an RDD for each review text and the reason it was rented for.
# 
# 
# #Creates an rdd with a tuple consisting of the following json lines:
# #Json.loads grabs the associated key values out, and appends them to a tuple.
# #If `rented for` is not found, it will not use the line
# rented_for_counts = rdd.map(lambda x: (json.loads(x).get('rented for', None), json.loads(x).get('review_text', None)))
# #print(rented_for_counts.take(10))
# 
# 
# 
# #We then need to count the number of words in each rented for key group.
# final_rdd = rented_for_counts.flatMap(lambda line: count_tuple(line))
# #print(final_rdd.take(6))
# 
# 
# 
# #This line takes the longest, so separate other transformations to below:
# combined_rdd = final_rdd.reduceByKey(lambda x, y: x + y)
# print(combined_rdd.take(10))
# 
# #Turn it into a total word count map we can reference later.
# total_word_map = sc.broadcast(combined_rdd.collectAsMap())
# 
# 
# 
# #Create a map for each occasion and the number of total ratings it has:
# 
# #First, give how many positive, negatives, and neutrals there are total., based on
# #occasions.
# 
# #Gives the total number of (occasion, rating) tuples
# rented_plus_rating = rdd.map(lambda x: (json.loads(x).get('rented for', None), json.loads(x)["rating"]))
# total_ratings = rented_plus_rating.map(lambda x: (rate_occasion(x), 1 ))
# 
# #Gives the total number of occasions, and the number of them equals the
# #number of total ratings within the occasion. This is our J
# just_occasion = rdd.map(lambda x: (json.loads(x).get('rented for', None)))
# just_occasion = just_occasion.map(lambda x: (x, 1))
# 
# 
# #Reduces both down into readable formats
# final_num_rating = total_ratings.reduceByKey(lambda x, y: x + y)
# total_occasion_count = just_occasion.reduceByKey(lambda x, y: x + y)
# 
# #Prints a result
# #print(final_num_rating.take(10))
# #print(total_occasion_count.take(10))
# 
# 
# #Broadcast the results as a map
# total_rating_map = sc.broadcast(final_num_rating.collectAsMap())
# total_occasion_count_map = sc.broadcast(total_occasion_count.collectAsMap())
# 
# 
# 
# #Group the tuple pair counts by their rented_for reason.
# grouped_rdd = combined_rdd.groupBy(lambda pair_count_tuple: pair_count_tuple[0][0])
# grouped_list = (grouped_rdd.take(20))
# print(grouped_list)
# 
# 
# 
# #Helper function to tuple map the confidences of a particular word.
# def process(row):
#     """Takes a row, and processes it from a map, returning confidences."""
#     lists = []
# 
#     #We will get an input in the format (('occasion', 'word', 'rating',), count)
#     occasion = row[0][0]
#     word = row[0][1]
#     rating = row[0][2]
# 
#     #Count is the number of words and "rating" in each dataset
#     count = row[1]
# 
#     #Final count calculates this, devided by the total number of the word occurrence.
#     final_count = count / total_word_map.value[(occasion, word)]
# 
#     #Append the result to a list for flatMapping:
#     lists.append(((occasion, word, rating), final_count))
# 
#     return lists
# 
# 
# #Create a new RDD.
# #Using the word, check the number of words that are split by within each reason, "positive", "negative", and "neutral".
# #From there, compare that number, with the total number.
# 
# rented_and_rating = rdd.map(lambda x: (json.loads(x).get('rented for', None), json.loads(x)['review_text'], json.loads(x)["rating"]))
# 
# #We then need to count the number of words in each rented for key group.
# rating_RDD = rented_and_rating.flatMap(lambda line: count_tuple_and_rating(line))
# 
# # Next, reduce them.
# reduced_rdd = rating_RDD.reduceByKey(lambda x, y: x + y)
# 
# # Rearrange the tuples to the desired format
# 
# #Create an RDD with all 3 variables, and the count of the word in question.
# result_rdd = reduced_rdd.map(lambda x: ((x[0][0], x[0][1], x[0][2]), x[1]))
# 
# 
# 
# 
# #Helper function to calculate Interests
# def calculate_interest(row):
#   lists = []
#   row = row[0]
#   key = (row[0][0], row[0][2])
#   occasion = row[0][0]
#   confidence = row[1]
#   word = row[0][1]
#   rating = row[0][2]
# 
#   #Get the total number
#   total_amount = total_rating_map.value[key]
# 
#   #this is given by the confidence minus the total number of J (ratings) from
#   #our total set of occasions. So if occasion 'party' has 600/1000 of its ratings
#   #as positive, we get a value of 0.6. For this, we will need a map.
#   interest = confidence - (total_rating_map.value[key] / total_occasion_count_map.value[occasion])
#   lists.append(((occasion, word, rating), interest))
#   return lists
# 
# #Now compare them in a new RDD to get the interests of each word within its own subset of ratings.
# 
# #Returns only the most frequent words. This will try to avoid single misspelt words
# resulting_rdd = result_rdd.filter(lambda x: x[1] > 3)
# 
# #Calculates confidences
# result_confidence = resulting_rdd.map(lambda row: process(row))
# print(result_confidence.take(30))
# 
# #I will now need to calculate the interest of each word for their occasiona nd rating.
# #This is given by the confidence - num(ratings) in all sets.
# 
# result_interest = result_confidence.map(lambda x: calculate_interest(x))
# print(result_interest.take(10))
# 
# #Sorts by interests
# sorted_rdd = result_interest.sortBy(lambda x: -x[0][1])
# 
# #Debugging line. Print out an ordered list of most interesting words, but not
# #grouped yet.
# results_q2a = sorted_rdd.takeOrdered(80, lambda kv: kv[0][0][2])
# print(results_q2a)
# 
# 
# 
# # Group the RDD elements by occasion
# grouped_rdd = sorted_rdd.groupBy(lambda x: (x[0][0][0], x[0][0][2]))
# #print(grouped_rdd.take(10))
# 
# # Define the order of ratings
# rating_order = ['Positive', 'Mixed', 'Negative']
# 
# # Define a helper function to get the index of a rating
# def get_rating_index(rating):
#     return rating_order.index(rating)
# 
# # Sorts groupedRDD by the index (Start with positive, then move down) (Due to spelling
# #And the spelling would not sort well).
# sorted_elements = sorted(grouped_rdd.collect(), key=lambda x: get_rating_index(x[0][1]))
# 
# # Iterate over the sorted elements and print the results
# for (occasion, rating), elements in sorted_elements:
#     # If our dataset contains less than 10 interesting and unique words, it is not going to be useful to us in the slightest, so only print useful ones.
#     if len(elements) > 10:
#         print("Occasion:", occasion.capitalize(), "-", rating)
#         top_10_elements = sorted(elements, key=lambda x: x[0][1], reverse=True)[:10]
# 
#         for element in top_10_elements:
#             print("Word:", element[0][0][1])
# 
#     print()
# 
# 
# 
#

"""#Setup the Google cloud cluster"""

!gcloud dataproc clusters create $CLUSTER --region=$REGION --bucket=$BUCKET --zone=$ZONE \
--master-machine-type=n1-standard-2 --worker-machine-type=n1-standard-1 \
--image-version=1.5 --max-age=30m --num-masters=3 --num-workers=2 \
--master-boot-disk-size=100GB --worker-boot-disk-size=100GB

"""Run the Job"""

!gcloud dataproc jobs submit pyspark --cluster=$CLUSTER --region=$REGION pyspark_data_project.py -- gs://$BUCKET/renttherunway_final_data.json

"""#Kill the cluster with aggressive malice."""

!gcloud dataproc clusters delete $CLUSTER --region=$REGION --quiet